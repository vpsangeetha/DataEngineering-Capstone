{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "A core responsibility of The National Travel and Tourism Office (NTTO) is to collect, analyze, and disseminate international travel and tourism statistics.\n",
    "\n",
    "NTTO's Board of Managers are charged with managing, improving, and expanding the system to fully account and report the impact of travel and tourism in the United States. The analysis results help to forcecast and operation, support make decision creates a positive climate for growth in travel and tourism by reducing institutional barriers to tourism, administers joint marketing efforts, provides official travel and tourism statistics, and coordinates efforts across federal agencies.\n",
    "\n",
    "The target of project is analysis the relationship between amount of travel immigration and weather duration by month of city.\n",
    "\n",
    "The source datas will be use to do data modeling are\n",
    "\n",
    "- I94 Immigration: The source data for I94 immigration data comes from US National Tourism and Trade Office. The actual source of the data is from https://travel.trade.gov/research/reports/i94/historical/2016.html. \n",
    "\n",
    "- World Temperature Data This dataset came from Kaggle. \n",
    "\n",
    "- I94_SAS_Labels_Descriptions.SAS to get validation dataset. We will use I94Port.txt as list of airport, city, state.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "I will be using the following pieces of data provided in the workspace\n",
    "1. I94 Immigration Data \n",
    "2. I94 SAS Label Descriptions\n",
    "3. Global Land Temperatures\n",
    "4. Airport Codes\n",
    "5. US City Demographics\n",
    "\n",
    "#### Technologies\n",
    "- Python\n",
    "- Spark\n",
    "- Pandas\n",
    "- AWS\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### I94 Immigration Data \n",
    "- cicid: Unique code assigned to the foreigners entering US\n",
    "- i94yr, i94month: Year and Month of entering the US\n",
    "- i94cit, i94res: Information about the country the person is coming from. Specific codes are available in the SAS Label Description file\n",
    "- i94port: US port of entry. Specific codes are available in the SAS Label Description file. This file gives details of which are the ports where immigration is allowed\n",
    "- arrdate: Date of arrival in the US\n",
    "- i94mode: Information about mode of entry. Eg: Air, Sea, Land. Specific codes are available in the SAS Label Description file\n",
    "- i94addr: State where foreigner is going.State codes are available in the SAS Label Description file\n",
    "- depdate: Date of departure from the US\n",
    "- i94bir: Specifies the age of the foreigner\n",
    "- i94visa: Information about the visa type. Eg; Business, Pleasure, Student. Specific codes are available in the SAS Label Description file\n",
    "- gender: gender of the foreigner\n",
    "- airline: Airline used to enter the US\n",
    "\n",
    "- count, dtadfile, visapost, occup, entdepa, entdepd, entdepu, matflag, biryear, dtaddto, insnum, admnum, fitno, visatype - Not used in this project\n",
    "\n",
    "##### I94 SAS Label Descriptions\n",
    "- I94PORT: Specifies airport code, city name, state code for port of entry\n",
    "- I94Mode: Specifies code and description for mode of travel - Air, Sea, Land\n",
    "- I94ADDR: Specifies immigration state code and state name\n",
    "- I94VISA: Specifies code and description for type of visa - Business, Pleasure, Student\n",
    "\n",
    "##### Global Land Temperatures\n",
    "- dt: Date the temperature was measured\n",
    "- AverageTemperature: Avg temperature captured\n",
    "- City: City where the temperature was measured\n",
    "- Country: Country where the temperature was measured\n",
    "- Latitude & Longitude: Latitude & Longitude of the location where the temperature was measured\n",
    "\n",
    "##### Airport Codes\n",
    "- ident: Unique identifier in the table\n",
    "- iso_country, iso_region: Country and region where the airport is located\n",
    "- type, name, elevation_ft, continent, municipality, gps_code, iata_code, coordinates - These fields are not used\n",
    "\n",
    "##### US City Demographics\n",
    "- City: City Name\n",
    "- State, State Code: State where the city is located\n",
    "- Foreign-born: May use this field\n",
    "- Median Age, Male Population, Female Population, Total Population, Number of Veterans, Race, Count - These fields are not used, Average Household Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import configparser\n",
    "import shutil\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld,\\\n",
    "    DoubleType as Dbl, StringType as Str, IntegerType as Int,\\\n",
    "    TimestampType as Timestamp, DateType as Date, LongType as Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "AWS_ACCESS_KEY_ID = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spark session\n"
     ]
    }
   ],
   "source": [
    "#Get the spark session\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\",AWS_ACCESS_KEY_ID)\\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\",AWS_SECRET_ACCESS_KEY)\\\n",
    "        .enableHiveSupport().getOrCreate()\n",
    "print(\"Created spark session\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data\n",
    "##### Read the Immigration data from the folder \"sas_data\"\n",
    "- Immigration Data has 3096313 rows of data\n",
    "\n",
    "\n",
    "##### Copy \"GlobalLandTemperaturesByCity.csv\" into the workspace and read the Global Temp data\n",
    "- Global Temp Data csv has 8599212 rows of data\n",
    "\n",
    "\n",
    "##### Read the SAS Labels file - I94_SAS_Labels_Descriptions.SAS \n",
    "- Extract the port Data into a temp csv file. Port data has 660 rows\n",
    "- Extract the state Data into a temp csv file. State data has 55 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readPortDataFromSASLabels(I94_SAS_Labels_Descriptions_String):\n",
    "    #I94Port Data - Extract the i94PortData and write them to csv files\n",
    "\n",
    "    #Extract I94 Port string by getting the substring from \"I94PORT\" to \";\"\n",
    "    I94Port_String = I94_SAS_Labels_Descriptions_String[I94_SAS_Labels_Descriptions_String.index('I94PORT'):]\n",
    "    I94Port_String = I94Port_String[:I94Port_String.index(';')]\n",
    "\n",
    "    I94Port_String_lines = I94Port_String.splitlines()\n",
    "    I94Port_Codes = []\n",
    "    for I94Port_String_line in I94Port_String_lines:\n",
    "        try:\n",
    "                I94Port_Code, I94Port_Value = I94Port_String_line.split('=')\n",
    "                I94Port_Code = I94Port_Code.strip().strip(\"'\").strip('\"')\n",
    "                I94Port_Value = I94Port_Value.strip().strip(\"'\").strip('\"').strip()\n",
    "                I94Port_Codes.append((I94Port_Code, I94Port_Value))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Write to CSV files\n",
    "    parent_dir = \"./Temp/\"\n",
    "    I94Port_csv = 'I94Port_csv'\n",
    "    path = os.path.join(parent_dir, I94Port_csv)\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok = True)\n",
    "    except OSError as error:\n",
    "        print(\"Directory could not be created\")\n",
    "\n",
    "    schema = R([\n",
    "            Fld(\"I94Port_Code\", Str()),\n",
    "            Fld(\"I94Port_Value\", Str())\n",
    "        ])\n",
    "\n",
    "    df_i94Port = spark.createDataFrame(\n",
    "            data=I94Port_Codes,\n",
    "            schema=schema\n",
    "        )\n",
    "\n",
    "    shutil.rmtree(path, ignore_errors=False, onerror=None)\n",
    "    df_i94Port.write.options(header='True', delimiter=',').csv(path)\n",
    "    df_i94Port = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(path)\n",
    "    df_i94Port.select(\"I94Port_Code\").distinct().count()\n",
    "    return df_i94Port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readAddrDataFromSASLabels(I94_SAS_Labels_Descriptions_String):\n",
    "    #I94Addr Data - Extract the i94AddrData and write them to csv files\n",
    "\n",
    "    #Extract I94 Addr string by getting the substring from \"I94Addr\" to \";\"\n",
    "    I94Addr_String = I94_SAS_Labels_Descriptions_String[I94_SAS_Labels_Descriptions_String.index('I94ADDR'):]\n",
    "    I94Addr_String = I94Addr_String[:I94Addr_String.index(';')]\n",
    "\n",
    "    I94Addr_String_lines = I94Addr_String.splitlines()\n",
    "    I94Addr_Codes = []\n",
    "    for I94Addr_String_line in I94Addr_String_lines:\n",
    "        try:\n",
    "                I94Addr_Code, I94Addr_Value = I94Addr_String_line.split('=')\n",
    "                I94Addr_Code = I94Addr_Code.strip().strip(\"'\").strip('\"')\n",
    "                I94Addr_Value = I94Addr_Value.strip().strip(\"'\").strip('\"').strip()\n",
    "                I94Addr_Codes.append((I94Addr_Code, I94Addr_Value))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Write to CSV files\n",
    "    parent_dir = \"./Temp/\"\n",
    "    I94Addr_csv = 'I94Addr_csv'\n",
    "    addr_path = os.path.join(parent_dir, I94Addr_csv)\n",
    "    try:\n",
    "        os.makedirs(addr_path, exist_ok = True)\n",
    "    except OSError as error:\n",
    "        print(\"Directory could not be created\")\n",
    "\n",
    "    schema = R([\n",
    "            Fld(\"I94Addr_Code\", Str()),\n",
    "            Fld(\"I94Addr_Value\", Str())\n",
    "        ])\n",
    "\n",
    "    df_i94Addr = spark.createDataFrame(\n",
    "            data=I94Addr_Codes,\n",
    "            schema=schema\n",
    "        )\n",
    "\n",
    "    shutil.rmtree(addr_path, ignore_errors=False, onerror=None)\n",
    "    df_i94Addr.write.options(header='True', delimiter=',').csv(addr_path)\n",
    "\n",
    "    df_i94Addr = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(addr_path)\n",
    "    df_i94Addr.select(\"I94Addr_Code\").distinct().count()\n",
    "    return df_i94Addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def writeDataFrameToCSV(dataframe, parent_dir, csvName):\n",
    "    immi_clean_path = os.path.join(parent_dir, csvName)\n",
    "    try:\n",
    "        os.makedirs(immi_clean_path, exist_ok = True)\n",
    "        print(\"Directory created\")\n",
    "    except OSError as error:\n",
    "        print(\"Directory could not be created\")\n",
    "    \n",
    "    shutil.rmtree(immi_clean_path, ignore_errors=False, onerror=None)\n",
    "    dataframe.write.options(header='True', delimiter=',').csv(immi_clean_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the SQL queries\n",
    "\n",
    "# Gather Data and Clean up Queries\n",
    "query_GetImmigrationCount = \\\n",
    "    \"SELECT count(*) \\\n",
    "        FROM i94immi_raw_dataset_table\"\n",
    "    \n",
    "query_GetImmigrationForArrivalByAir = \\\n",
    "    \"SELECT * \\\n",
    "        FROM i94immi_raw_dataset_table \\\n",
    "        WHERE i94mode == 1.0\"\n",
    "    \n",
    "query_GetImmigrationForSpecificGender = \\\n",
    "    \"SELECT * \\\n",
    "        FROM i94immi_raw_dataset_table \\\n",
    "        WHERE gender IN ('F', 'M')\"\n",
    "\n",
    "query_GetImmigrationWithoutNullAddr = \\\n",
    "    \"SELECT * \\\n",
    "        FROM i94immi_raw_dataset_table \\\n",
    "        WHERE i94addr IS NOT NULL\"\n",
    "\n",
    "query_GetImmigrationWithVisaName = \\\n",
    "    \"SELECT *, CASE \\\n",
    "        WHEN i94visa = 1 THEN 'Business' \\\n",
    "        WHEN i94visa = 2 THEN 'Pleasure' \\\n",
    "        WHEN i94visa = 3 THEN 'Student' \\\n",
    "        END AS i94immi_visatype \\\n",
    "        FROM i94immi_raw_dataset_table\"\n",
    "\n",
    "query_GetImmigrationWithDatTimeArrivalDate = \\\n",
    "    \"SELECT *, date_add(to_date('1960-01-01'), arrdate) AS i94immi_arrival_date \\\n",
    "        FROM i94immi_raw_dataset_table\"\n",
    "\n",
    "query_GetImmigrationWithDatTimeDepartureDate = \\\n",
    "    \"SELECT *, date_add(to_date('1960-01-01'), depdate) AS i94immi_dep_date \\\n",
    "        FROM i94immi_raw_dataset_table\"\n",
    "\n",
    "query_GetImmigrationCleanedUp = \\\n",
    "    \"SELECT \\\n",
    "            cicid, \\\n",
    "            i94cit, \\\n",
    "            i94res, \\\n",
    "            i94port, \\\n",
    "            i94immi_arrival_date, \\\n",
    "            i94yr, \\\n",
    "            i94mon, \\\n",
    "            i94mode, \\\n",
    "            i94addr, \\\n",
    "            i94immi_dep_date, \\\n",
    "            i94bir, \\\n",
    "            i94visa, \\\n",
    "            gender, \\\n",
    "            airline, \\\n",
    "            admnum, \\\n",
    "            fltno, \\\n",
    "            i94immi_visatype, \\\n",
    "            visatype \\\n",
    "        FROM i94immi_raw_dataset_table\"\n",
    "\n",
    "query_GetImmigrationAll = \\\n",
    "    \"SELECT * \\\n",
    "        FROM i94immi_raw_dataset_table\"\n",
    "\n",
    "\n",
    "# Queries to create the fact and dimention tables\n",
    "query_CreateFactImmigrationTable = \\\n",
    "    \"SELECT \\\n",
    "        cicid AS i94immi_cicid, \\\n",
    "        i94cit AS i94immi_citizenship_country_code, \\\n",
    "        i94res AS i94immi_residence_country_code, \\\n",
    "        i94port AS i94immi_arrival_port_code, \\\n",
    "        i94immi_arrival_date, \\\n",
    "        i94yr AS i94immi_arrival_year, \\\n",
    "        i94mon AS i94immi_arrival_month, \\\n",
    "        i94mode AS i94immi_airline_mode_code, \\\n",
    "        i94addr AS i94immi_state_code, \\\n",
    "        i94immi_dep_date, \\\n",
    "        i94bir AS i94immi_foreigner_age, \\\n",
    "        i94visa AS i94immi_visatype_number, \\\n",
    "        gender AS i94immi_foreigner_sex, \\\n",
    "        fltno AS i94immi_flight_code, \\\n",
    "        visatype AS i94immi_visatype_code, \\\n",
    "        i94immi_visatype \\\n",
    "    FROM fact_i94ImmigrationData\"\n",
    "\n",
    "query_CreateDimensionFlightTable = \\\n",
    "    \"SELECT \\\n",
    "            fltno as flight_number, \\\n",
    "            airline as flight_brand, \\\n",
    "            i94port as flight_airport_city \\\n",
    "        FROM dim_Flight\"\n",
    "\n",
    "query_CreateDimensionVisaTable = \\\n",
    "        \"SELECT \\\n",
    "            visatype as visa_visatype_code, \\\n",
    "            i94visa as visa_visatype_number, \\\n",
    "            i94immi_visatype as visa_type_name \\\n",
    "        FROM dim_Visa\"\n",
    "\n",
    "query_CreateDimensionAddressTable = \\\n",
    "        \"SELECT \\\n",
    "            I94Addr_Code as address_state_code, \\\n",
    "            I94Addr_Value as address_state_name \\\n",
    "        FROM dim_Address\"\n",
    "\n",
    "query_CreateDimensionPortTable = \\\n",
    "        \"SELECT \\\n",
    "            I94Port_Code as port_code, \\\n",
    "            I94Port_City as port_city_name, \\\n",
    "            I94Port_State as port_state_code \\\n",
    "        FROM dim_Port\"\n",
    "\n",
    "query_CreateDimensionForeignerTable = \\\n",
    "        \"SELECT \\\n",
    "            cicid AS foreigner_cicid, \\\n",
    "            i94cit AS foreigner_cit_country_code, \\\n",
    "            i94res AS foreigner_res_country_code, \\\n",
    "            i94port AS foreigner_arrival_port_code, \\\n",
    "            i94immi_arrival_date AS foreigner_arrival_date, \\\n",
    "            i94addr AS foreigner_state_code, \\\n",
    "            i94bir AS foreigner_age, \\\n",
    "            gender AS foreigner_sex, \\\n",
    "            i94visa AS foreigner_visatype_code \\\n",
    "        FROM dim_Foreigner\"\n",
    "\n",
    "query_CreateFactTempTemperatureTable = \\\n",
    "        \"SELECT  \\\n",
    "            dt, \\\n",
    "            MONTH(Temperature_Table.dt) as month, \\\n",
    "            YEAR(Temperature_Table.dt) as year, \\\n",
    "            city, \\\n",
    "            averagetemperature, \\\n",
    "            averagetemperatureuncertainty \\\n",
    "        FROM Temperature_Table\"\n",
    "\n",
    "query_UpdateFactTempTemperatureTableByCity = \\\n",
    "    \"SELECT  \\\n",
    "        city, \\\n",
    "        month, \\\n",
    "        averagetemperature, \\\n",
    "        averagetemperatureuncertainty, \\\n",
    "        year, \\\n",
    "        dt \\\n",
    "    FROM Temperature_Table \\\n",
    "    GROUP BY city, month, year, dt, averagetemperature, averagetemperatureuncertainty \\\n",
    "    ORDER BY year\"\n",
    "\n",
    "query_GetAllFromFactTempTemperatureTable = \\\n",
    "    \"SELECT * FROM Temperature_Table\"\n",
    "\n",
    "query_CreateFactCitiesTemperatureTable = \\\n",
    "        \"SELECT \\\n",
    "            averagetemperature AS worldtemp_average, \\\n",
    "            city AS worldtemp_city, \\\n",
    "            month AS worldtemp_month, \\\n",
    "            dt AS worldtemp_measuredate \\\n",
    "        FROM fact_Cities_Temperature\"\n",
    "\n",
    "# Queries for statistics\n",
    "query_GetImmigrationByGender = \\\n",
    "    \"SELECT \\\n",
    "        i94immi_foreigner_sex AS gender, \\\n",
    "        count(*) AS foreigner_count_bygender \\\n",
    "        FROM Fact_i94ImmigrationData_View as Fact_i94ImmigrationData \\\n",
    "        GROUP BY gender \\\n",
    "        LIMIT 5\"\n",
    "\n",
    "query_GetImmigrationByAge = \\\n",
    "    \"SELECT \\\n",
    "        i94immi_foreigner_age AS age, \\\n",
    "        count(*) AS foreigner_count_byage \\\n",
    "        FROM Fact_i94ImmigrationData_View as Fact_i94ImmigrationData \\\n",
    "        GROUP BY age \\\n",
    "        ORDER BY foreigner_count_byage desc \\\n",
    "        LIMIT 5\"\n",
    "\n",
    "query_GetImmigrationByCity = \\\n",
    "    \"SELECT \\\n",
    "            Fact_i94ImmigrationData.i94immi_arrival_port_code as airport_code, \\\n",
    "            Dim_portData.port_city_name as city_name, \\\n",
    "            Fact_i94ImmigrationData.i94immi_flight_code as flight_traffic \\\n",
    "        FROM Fact_i94ImmigrationData_View as Fact_i94ImmigrationData \\\n",
    "        JOIN Dim_portData_View as Dim_portData \\\n",
    "            ON Dim_portData.port_code = Fact_i94ImmigrationData.i94immi_arrival_port_code \\\n",
    "        GROUP BY city_name, airport_code, flight_traffic\"\n",
    "\n",
    "query_GetCityCountByImmigration = \\\n",
    "    \"SELECT count(*) FROM ( \\\n",
    "        SELECT DISTINCT city_name FROM \\\n",
    "        ImmigrationStatisticsByCity \\\n",
    "    )\"\n",
    "\n",
    "query_GetForeignerCountByVisa = \\\n",
    "    \"SELECT \\\n",
    "        Dim_visaData.visa_type_name AS visa_name, \\\n",
    "        count(*) \\\n",
    "    FROM Fact_i94ImmigrationData_View as Fact_i94ImmigrationData \\\n",
    "    JOIN Dim_visaData_View as Dim_visaData \\\n",
    "        ON Dim_visaData.visa_visatype_code = Fact_i94ImmigrationData.i94immi_visatype_code \\\n",
    "    GROUP BY visa_name\"\n",
    "\n",
    "query_GetImmigrationNumbersByCityPrep = \\\n",
    "    \"SELECT \\\n",
    "        Fact_i94ImmigrationData.i94immi_cicid as travel_cicid, \\\n",
    "        Fact_i94ImmigrationData.i94immi_arrival_port_code as airport_code, \\\n",
    "        LOWER(Dim_portData.port_city_name) as city_name, \\\n",
    "        Fact_i94ImmigrationData.i94immi_arrival_month as travel_month, \\\n",
    "        Fact_i94ImmigrationData.i94immi_arrival_date as travel_date \\\n",
    "    FROM Fact_i94ImmigrationData_View as Fact_i94ImmigrationData \\\n",
    "    JOIN Dim_portData_View as Dim_portData \\\n",
    "    ON Dim_portData.port_code = Fact_i94ImmigrationData.i94immi_arrival_port_code\"\n",
    "\n",
    "query_GetCityAirportForeignerCount = \\\n",
    "    \"SELECT \\\n",
    "        city_name, \\\n",
    "        airport_code, \\\n",
    "        COUNT(travel_cicid) as total_foreigners \\\n",
    "    FROM Immigration_City \\\n",
    "    GROUP BY city_name, airport_code \\\n",
    "    ORDER BY total_foreigners DESC\"\n",
    "\n",
    "query_GetTop5CitiesByForeignerCount = \\\n",
    "    \"SELECT * from Immigration_City \\\n",
    "    LIMIT 5\"\n",
    "\n",
    "query_GetCityTemperature = \\\n",
    "            \"SELECT \\\n",
    "                worldtemp_average, \\\n",
    "                LOWER(worldtemp_city) AS worldtemp_city, \\\n",
    "                worldtemp_month, \\\n",
    "                worldtemp_measuredate \\\n",
    "            FROM Fact_temperatureData_View \\\n",
    "            WHERE worldtemp_month = 4\"\n",
    "\n",
    "query_GetCityAverageTemperature = \\\n",
    "                \"SELECT \\\n",
    "                    worldtemp_city, \\\n",
    "                    AVG(worldtemp_average) as worldtemp_Average_For_City \\\n",
    "                FROM City_Temperature \\\n",
    "                GROUP BY worldtemp_city \\\n",
    "                ORDER BY worldtemp_Average_For_City DESC\"\n",
    "\n",
    "query_GetCityImmigrationByTemperature = \\\n",
    "        \"SELECT \\\n",
    "            Immigration_City.city_name as city_name, \\\n",
    "                City_Temperature.worldtemp_Average_For_City as temperature, \\\n",
    "                Immigration_City.total_foreigners as total_foreigners \\\n",
    "        FROM Immigration_City \\\n",
    "        LEFT JOIN City_Temperature \\\n",
    "        ON City_Temperature.worldtemp_city = Immigration_City.city_name\"\n",
    "\n",
    "query_GetTop5CityImmigrationByTemperature = \\\n",
    "    \"SELECT * FROM Immigration_City_Temperature \\\n",
    "    LIMIT 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the Immigration data\n",
    "i94_immi_df=spark.read.parquet(\"sas_data\")\n",
    "i94_immi_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the Global Temp data\n",
    "world_temp_file = 'GlobalLandTemperaturesByCity.csv'\n",
    "world_temp_df = pd.read_csv(world_temp_file,sep=\",\")\n",
    "len(world_temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read SAS Labels file\n",
    "with open('I94_SAS_Labels_Descriptions.SAS') as I94_SAS_Labels_Descriptions:\n",
    "    I94_SAS_Labels_Descriptions_String = I94_SAS_Labels_Descriptions.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_i94Port = readPortDataFromSASLabels(I94_SAS_Labels_Descriptions_String)\n",
    "df_i94Port = df_i94Port.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_i94Addr = readAddrDataFromSASLabels(I94_SAS_Labels_Descriptions_String)\n",
    "df_i94Addr = df_i94Addr.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "Document steps necessary to clean the data\n",
    "\n",
    "##### I94 Immigration Data\n",
    "- Data used - ./sas_data/i94immi_raw_dataset\n",
    "- Create Spark SQL table\n",
    "- Filter out only immigration records for arrival by air\n",
    "- Filter out gender column by values of 'M' and 'F'\n",
    "- Clean up null values for I94Addr\n",
    "- Replace visatype number by visatype category\n",
    "- Convert arrival and dep dates to datetime value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 3096313|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "# Read & Clean up the immigration raw dataset\n",
    "i94immi_raw_dataset = './sas_data'\n",
    "i94immi_raw_dataset_df = spark.read.parquet(i94immi_raw_dataset)\n",
    "\n",
    "# Create Spark SQL table for clean up\n",
    "i94immi_raw_dataset_df.createOrReplaceTempView('i94immi_raw_dataset_table')\n",
    "spark.sql(query_GetImmigrationCount).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2994505|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filter out only immigration records for arrival by air\n",
    "spark.sql(query_GetImmigrationForArrivalByAir).createOrReplaceTempView(\"i94immi_raw_dataset_table\")\n",
    "spark.sql(query_GetImmigrationCount).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2581480|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter out gender column by values of 'M' and 'F'\n",
    "spark.sql(query_GetImmigrationForSpecificGender).createOrReplaceTempView(\"i94immi_raw_dataset_table\")\n",
    "spark.sql(query_GetImmigrationCount).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2485241|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean up null values for I94Addr\n",
    "# Drop NULL value on arrival state\n",
    "spark.sql(query_GetImmigrationWithoutNullAddr).createOrReplaceTempView(\"i94immi_raw_dataset_table\")\n",
    "spark.sql(query_GetImmigrationCount).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace visatype number by visatype category\n",
    "spark.sql(query_GetImmigrationWithVisaName).createOrReplaceTempView(\"i94immi_raw_dataset_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2485241|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert arrival and departure dates to DateTime\n",
    "spark.sql(query_GetImmigrationWithDatTimeArrivalDate).createOrReplaceTempView(\"i94immi_raw_dataset_table\")\n",
    "spark.sql(query_GetImmigrationWithDatTimeDepartureDate).createOrReplaceTempView(\"i94immi_raw_dataset_table\")\n",
    "spark.sql(query_GetImmigrationCount).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2485241"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(query_GetImmigrationCleanedUp).createOrReplaceTempView('i94immi_raw_dataset_table')\n",
    "df_i94immi_cleaned = spark.sql(query_GetImmigrationAll)\n",
    "df_i94immi_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2485241"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write cleaned up immigration data to csv \n",
    "writeDataFrameToCSV(df_i94immi_cleaned, './Clean/', 'I94Immi_Data_Clean_csv')\n",
    "immi_clean_path = os.path.join('./Clean/', 'I94Immi_Data_Clean_csv')\n",
    "\n",
    "df_i94immi_cleaned = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(immi_clean_path)\n",
    "df_i94immi_cleaned.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Flight Data\n",
    "- Data used - ./i94Immi_Data_Clean_csv\n",
    "- Get unique airline information by i94immi_flight_code, airline,  i94immi_arrival_port_code\n",
    "- Remove duplicates for i94immi_flight_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2485241"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather Flight Data from the Immigration file\n",
    "df_flightinfo = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(immi_clean_path)\n",
    "df_flightinfo.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6314"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up Flight Data\n",
    "df_flightinfo = df_flightinfo.select(['fltno', 'airline', 'i94port'])\n",
    "df_flightinfo = df_flightinfo.dropDuplicates(['fltno'])\n",
    "df_flightinfo.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6314"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write cleaned up flight data to csv \n",
    "writeDataFrameToCSV(df_flightinfo, './Clean/', 'Flight_Data_Clean_csv')\n",
    "flight_clean_path = os.path.join('./Clean/', 'Flight_Data_Clean_csv')\n",
    "\n",
    "df_flightinfo = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(flight_clean_path)\n",
    "df_flightinfo.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Foreigner Dataset\n",
    "- Data used - ./i94Immi_Data_Clean_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2485241"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather Foreigner Data from the Immigration file\n",
    "df_foreignerinfo = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(immi_clean_path)\n",
    "df_foreignerinfo.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2485241"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_foreignerinfo = df_foreignerinfo.select(\n",
    "        ['cicid','i94cit','i94res','i94port','i94immi_arrival_date',\n",
    "            'i94mode','i94addr','i94bir','i94visa','gender'])\n",
    "df_foreignerinfo.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2485241"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write cleaned up foreigner data to csv \n",
    "writeDataFrameToCSV(df_foreignerinfo, './Clean/', 'Foreigner_Data_Clean_csv')\n",
    "foreigner_clean_path = os.path.join('./Clean/', 'Foreigner_Data_Clean_csv')\n",
    "\n",
    "df_foreignerinfo = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(foreigner_clean_path)\n",
    "df_foreignerinfo.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Visa Dataset\n",
    "- Data used - ./i94Immi_Data_Clean_csv\n",
    "- Select columns i94visa,i94immi_visatype,visatype\n",
    "- Drop duplicates by i94visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2485241"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather Visa Data from the Immigration file\n",
    "df_visainfo = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(immi_clean_path)\n",
    "df_visainfo.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2485241"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_visainfo = df_visainfo.select(['i94visa','i94immi_visatype','visatype'])\n",
    "df_visainfo.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------+\n",
      "|i94visa|i94immi_visatype|visatype|\n",
      "+-------+----------------+--------+\n",
      "|    3.0|         Student|      F2|\n",
      "|    1.0|        Business|     GMB|\n",
      "|    2.0|        Pleasure|      B2|\n",
      "|    3.0|         Student|      F1|\n",
      "|    2.0|        Pleasure|     CPL|\n",
      "|    1.0|        Business|      I1|\n",
      "|    1.0|        Business|      WB|\n",
      "|    3.0|         Student|      M1|\n",
      "|    1.0|        Business|      B1|\n",
      "|    2.0|        Pleasure|      WT|\n",
      "|    3.0|         Student|      M2|\n",
      "|    2.0|        Pleasure|      CP|\n",
      "|    2.0|        Pleasure|     GMT|\n",
      "|    1.0|        Business|      E1|\n",
      "|    1.0|        Business|       I|\n",
      "|    1.0|        Business|      E2|\n",
      "|    2.0|        Pleasure|     SBP|\n",
      "+-------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Clean up Visa Info\n",
    "df_visainfo = df_visainfo.dropDuplicates(['visatype'])\n",
    "df_visainfo.count()\n",
    "df_visainfo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write cleaned up visa data to csv \n",
    "writeDataFrameToCSV(df_visainfo, './Clean/', 'Visa_Data_Clean_csv')\n",
    "visa_clean_path = os.path.join('./Clean/', 'Visa_Data_Clean_csv')\n",
    "\n",
    "df_visainfo = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(visa_clean_path)\n",
    "df_visainfo.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### World Temperature dataset\n",
    "- Data used ./GlobalLandWorldTemperatures.csv\n",
    "- Filter data to just look at US cities\n",
    "- Filter data to just look at data from 1960 to match immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Gather the world temerature Data & Clean it up\n",
    "US_Cities_Temp_CSV = 'GlobalLandTemperaturesByCity.csv'\n",
    "df_test = pd.read_csv(US_Cities_Temp_CSV, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_us_cities_temperature = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(US_Cities_Temp_CSV)\n",
    "df_us_cities_temperature.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687289"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the data by US Cities\n",
    "df_us_cities_temperature = df_us_cities_temperature[df_us_cities_temperature['Country']=='United States']\n",
    "df_us_cities_temperature.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "|                 dt|AverageTemperature|AverageTemperatureUncertainty|   City|      Country|Latitude|Longitude|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "|1960-01-01 00:00:00| 5.242999999999999|                          0.3|Abilene|United States|  32.95N|  100.53W|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the data by date\n",
    "df_us_cities_temperature = df_us_cities_temperature[df_us_cities_temperature['dt']>\"1960-01-01\"]\n",
    "df_us_cities_temperature.count()\n",
    "df_us_cities_temperature.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created\n",
      "+-------------------+------------------+-----------------------------+-----------+-------------+--------+---------+\n",
      "|                 dt|AverageTemperature|AverageTemperatureUncertainty|       City|      Country|Latitude|Longitude|\n",
      "+-------------------+------------------+-----------------------------+-----------+-------------+--------+---------+\n",
      "|1960-01-01 00:00:00|7.5329999999999995|           0.7979999999999999|Los Angeles|United States|  34.56N|  118.70W|\n",
      "+-------------------+------------------+-----------------------------+-----------+-------------+--------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write cleaned up temperature data to csv \n",
    "writeDataFrameToCSV(df_us_cities_temperature, './Clean/', 'Temperature_Data_Clean_csv')\n",
    "temperature_clean_path = os.path.join('./Clean/', 'Temperature_Data_Clean_csv')\n",
    "\n",
    "df_us_cities_temperature = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(temperature_clean_path)\n",
    "df_us_cities_temperature.count()\n",
    "df_us_cities_temperature.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### i94 SAS Label Descriptions - Port\n",
    "- Data used df_i94Port\n",
    "- Split into port, city, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Split to port, city, state\n",
    "df_i94Port[\"I94Port_City\"] = df_i94Port[\"I94Port_Value\"].str.split(\",\").str.get(0)\n",
    "df_i94Port[\"I94Port_State\"] = df_i94Port[\"I94Port_Value\"].str.split(\",\").str.get(1)\n",
    "df_i94Port = df_i94Port[['I94Port_Code', 'I94Port_City', 'I94Port_State']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Remove empty values for state\n",
    "df_i94Port = df_i94Port.dropna(subset = [\"I94Port_State\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_i94Port = spark.createDataFrame(df_i94Port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "583"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write cleaned up port data to csv \n",
    "writeDataFrameToCSV(df_i94Port, './Clean/', 'Port_Data_Clean_csv')\n",
    "port_clean_path = os.path.join('./Clean/', 'Port_Data_Clean_csv')\n",
    "\n",
    "df_i94Port = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(port_clean_path)\n",
    "df_i94Port.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### i94 SAS Label Descriptions - Addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_i94Addr = spark.createDataFrame(df_i94Addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write cleaned up state data to csv \n",
    "writeDataFrameToCSV(df_i94Addr, './Clean/', 'Addr_Data_Clean_csv')\n",
    "addr_clean_path = os.path.join('./Clean/', 'Addr_Data_Clean_csv')\n",
    "\n",
    "df_i94Addr = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(addr_clean_path)\n",
    "df_i94Addr.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The goal here is to utilize the immigration data and the world temperature data provided to get statistics for immigration. the various statistics captured here are\n",
    "1. Immigration by city\n",
    "2. Immigration by gender\n",
    "3. Immigration by age\n",
    "4. Immigration by visa type\n",
    "5. Immmigration by city temperature\n",
    "\n",
    "I am using a star schema here with the following tables\n",
    "1. Fact Tables\n",
    "    1. Immigration Table\n",
    "    2. World Temperature Table\n",
    "2. Dimension tables\n",
    "    1. Flight Table\n",
    "    2. Visa Table\n",
    "    3. Address Table\n",
    "    4. Port Table\n",
    "    5. Foreigner Table\n",
    "    \n",
    "##### Details of the tables\n",
    "###### Immigration Fact Table\n",
    "- Table Name: fact_i94ImmigrationData\n",
    "- Columns:\n",
    "    - i94immi_cicid,\n",
    "    - i94immi_citizenship_country_code,\n",
    "    - i94immi_residence_country_code,\n",
    "    - i94immi_arrival_port_code,\n",
    "    - i94immi_arrival_date,\n",
    "    - i94immi_arrival_year,\n",
    "    - i94immi_arrival_month,\n",
    "    - i94immi_airline_mode_code,\n",
    "    - i94immi_state_code,\n",
    "    - i94immi_dep_date,\n",
    "    - i94bir AS i94immi_foreigner_age,\n",
    "    - i94immi_visatype_number,\n",
    "    - i94immi_foreigner_sex,\n",
    "    - i94immi_flight_code,\n",
    "    - i94immi_visatype_code,\n",
    "    - i94immi_visatype\n",
    "\n",
    "###### World Temperature Fact Table\n",
    "- Table Name: fact_Cities_Temperature\n",
    "- Columns:\n",
    "    - averagetemperature\n",
    "    - worldtemp_city\n",
    "    - worldtemp_month\n",
    "    - worldtemp_measuredate\n",
    "\n",
    "###### Flight Dimension Table\n",
    "- Table Name: dim_Flight\n",
    "- Columns:\n",
    "    - flight_number\n",
    "    - flight_brand\n",
    "    - flight_airport_city\n",
    "\n",
    "###### Visa Dimension Table\n",
    "- Table Name: dim_Visa\n",
    "- Columns:\n",
    "    - visa_visatype_code\n",
    "    - visa_visatype_number\n",
    "    - visa_type_name\n",
    "\n",
    "###### Address Dimension Table\n",
    "- Table Name: dim_Address\n",
    "- Columns:\n",
    "    - address_state_code\n",
    "    - address_state_name\n",
    "\n",
    "###### Port Dimension Table\n",
    "- Table Name: dim_Port\n",
    "- Columns:\n",
    "    - port_code\n",
    "    - port_city_name\n",
    "    - port_state_code\n",
    "\n",
    "###### Foreigner Dimension Table\n",
    "- Table Name: dim_Foreigner\n",
    "- Columns:\n",
    "    - foreigner_cicid\n",
    "    - foreigner_cit_country_code\n",
    "    - foreigner_res_country_code\n",
    "    - foreigner_arrival_port_code\n",
    "    - foreigner_arrival_date\n",
    "    - foreigner_state_code\n",
    "    - foreigner_age\n",
    "    - foreigner_sex\n",
    "    - foreigner_visatype_code\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "\n",
    "##### Locate the raw data sources\n",
    "- I94_SAS_Labels_Descriptions.SAS  \n",
    "- sas_data\n",
    "- GlobalLandTemperaturesByCity.csv  metastore_db \n",
    "\n",
    "##### Read the data into dataframes\n",
    "- Read the port and state data into temp csv files and load them to data frames\n",
    "\t- df_i94Port\n",
    "\t- df_i94Addr\n",
    "- Read the immigration data into a data frame - i94_immi_df\n",
    "- Read the global temperature data into a data frame - world_temp_df\n",
    "\n",
    "##### Clean up the data and write the cleaned up data into csv files\n",
    "- Load the cleaned up immigration data into a data frame - df_i94immi_cleaned and write it into a csv file - I94Immi_Data_Clean_csv\n",
    "- Load the cleaned up flight data into a data frame - df_flightinfo and write it into a csv file - Flight_Data_Clean_csv\n",
    "- Load the cleaned up foreigner data into a data frame - df_foreignerinfo and write it into a csv file - Foreigner_Data_Clean_csv\n",
    "- Load the cleaned up visa data into a data frame - df_visainfo and write it into a csv file - Visa_Data_Clean_csv\n",
    "- Load the cleaned up world temperature data into a data frame - df_us_cities_temperature and write it into a csv file - Temperature_Data_Clean_csv\n",
    "- Load the cleaned up port data into a data frame - df_i94Port and write it into a csv file - Port_Data_Clean_csv\n",
    "- Load the cleaned up address data into a data frame - df_i94Addr and write it into a csv file - Addr_Data_Clean_csv\n",
    "\n",
    "##### Apply the required transforms and load the parquet files into S3\n",
    "- Immigration fact table - fact_i94ImmigrationData.parquet\n",
    "- Flight dimension table - dim_Flight.parquet\n",
    "- Visa dimension table - dim_Visa.parquet\n",
    "- Address dimension table - dim_Address.parquet\n",
    "- Port dimension table - dim_Port.parquet\n",
    "- Foreigner dimension table - dim_Foreigner.parquet\n",
    "- Cities temperature fact table - fact_Cities_Temperature.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# S3 Location for parquet files\n",
    "parquet_files_directory = 's3a://spcapstone1/parquetFiles/'\n",
    "\n",
    "# Local Location for parquet files\n",
    "#parquet_files_directory = './parquetFiles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Immigration Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_i94immi_cleaned.createOrReplaceTempView('fact_i94ImmigrationData')\n",
    "fact_i94ImmigrationData = spark.sql(query_CreateFactImmigrationTable)\n",
    "\n",
    "immi_parquet_path = parquet_files_directory + 'fact_i94ImmigrationData.parquet'\n",
    "fact_i94ImmigrationData.write.mode(\"overwrite\").parquet(immi_parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Flight dim table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_flightinfo.createOrReplaceTempView('dim_Flight')\n",
    "dim_Flight = spark.sql(query_CreateDimensionFlightTable)\n",
    "flight_parquet_path = parquet_files_directory + 'dim_flightData.parquet'\n",
    "dim_Flight.write.mode(\"overwrite\").parquet(flight_parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Visa dim table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_visainfo.createOrReplaceTempView('dim_Visa')\n",
    "dim_Visa = spark.sql(query_CreateDimensionVisaTable)\n",
    "visa_parquet_path = parquet_files_directory + 'dim_visaData.parquet'\n",
    "dim_Visa.write.mode(\"overwrite\").parquet(visa_parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Address dim Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_i94Addr.createOrReplaceTempView('dim_Address')\n",
    "dim_Address = spark.sql(query_CreateDimensionAddressTable)\n",
    "address_parquet_path = parquet_files_directory + 'dim_addressData.parquet'\n",
    "dim_Address.write.mode(\"overwrite\").parquet(address_parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Port dim table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_i94Port.createOrReplaceTempView('dim_Port')\n",
    "dim_Port = spark.sql(query_CreateDimensionPortTable)\n",
    "port_parquet_path = parquet_files_directory + 'dim_portData.parquet'\n",
    "dim_Port.write.mode(\"overwrite\").parquet(port_parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Foreigner dim table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_foreignerinfo.createOrReplaceTempView('dim_Foreigner')\n",
    "dim_Foreigner = spark.sql(query_CreateDimensionForeignerTable)\n",
    "foreigner_parquet_path = parquet_files_directory + 'dim_foreignerData.parquet'\n",
    "dim_Foreigner.write.mode(\"overwrite\").parquet(foreigner_parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Temperature fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_us_cities_temperature.createOrReplaceTempView('Temperature_Table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(query_CreateFactTempTemperatureTable).createOrReplaceTempView('Temperature_Table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Grouping average temperature by city \n",
    "spark.sql(query_UpdateFactTempTemperatureTableByCity).createOrReplaceTempView('Temperature_Table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_us_cities_temperature = spark.sql(query_GetAllFromFactTempTemperatureTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_us_cities_temperature.createOrReplaceTempView('fact_Cities_Temperature')\n",
    "fact_Cities_Temperature = spark.sql(query_CreateFactCitiesTemperatureTable)\n",
    "cities_Temperature__parquet_path = parquet_files_directory + 'fact_Cities_Temperature.parquet'\n",
    "fact_Cities_Temperature.write.mode(\"overwrite\").parquet(cities_Temperature__parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here\n",
    "\n",
    "#parquet_files_directory = 's3a://spcapstone1/parquetFiles/'\n",
    "parquet_files_directory = './parquetFiles/'\n",
    "\n",
    "fact_i94ImmigrationDataPath = parquet_files_directory + 'fact_i94ImmigrationData.parquet'\n",
    "df_Fact_i94ImmigrationData = spark.read.parquet(fact_i94ImmigrationDataPath)\n",
    "df_Fact_i94ImmigrationData.createOrReplaceTempView('Fact_i94ImmigrationData_View')\n",
    "\n",
    "dim_portDataPath = parquet_files_directory + 'dim_portData.parquet'\n",
    "df_Dim_portData = spark.read.parquet(dim_portDataPath)\n",
    "df_Dim_portData.createOrReplaceTempView('Dim_portData_View')\n",
    "\n",
    "dim_visaDataPath = parquet_files_directory + 'dim_visaData.parquet'\n",
    "df_Dim_visaData = spark.read.parquet(dim_visaDataPath)\n",
    "df_Dim_visaData.createOrReplaceTempView('Dim_visaData_View')\n",
    "\n",
    "fact_temperatureDataPath = parquet_files_directory + 'fact_Cities_Temperature.parquet'\n",
    "df_fact_temperatureData = spark.read.parquet(fact_temperatureDataPath)\n",
    "df_fact_temperatureData.createOrReplaceTempView('Fact_temperatureData_View')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration by gender:\n",
      "+------+------------------------+\n",
      "|gender|foreigner_count_bygender|\n",
      "+------+------------------------+\n",
      "|     F|                 1210561|\n",
      "|     M|                 1274680|\n",
      "+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Immigration by gender:\")\n",
    "spark.sql(query_GetImmigrationByGender).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration by age:\n",
      "+----+---------------------+\n",
      "| age|foreigner_count_byage|\n",
      "+----+---------------------+\n",
      "|30.0|                58560|\n",
      "|31.0|                57159|\n",
      "|33.0|                56957|\n",
      "|34.0|                56841|\n",
      "|32.0|                56417|\n",
      "+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Immigration by age:\")\n",
    "spark.sql(query_GetImmigrationByAge).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(query_GetImmigrationByCity).createOrReplaceTempView('ImmigrationStatisticsByCity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Cities where foreigners arrived:\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     161|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Number of Cities where foreigners arrived:\")\n",
    "spark.sql(query_GetCityCountByImmigration).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foreigner count by visa:\n",
      "+---------+--------+\n",
      "|visa_name|count(1)|\n",
      "+---------+--------+\n",
      "| Pleasure| 2057664|\n",
      "|  Student|   38732|\n",
      "| Business|  388845|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Foreigner count by visa:\")\n",
    "spark.sql(query_GetForeignerCountByVisa).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(query_GetImmigrationNumbersByCityPrep).createOrReplaceTempView('Immigration_City')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(query_GetCityAirportForeignerCount).createOrReplaceTempView('Immigration_City')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities with most foreigner arrival:\n",
      "+----------------+------------+----------------+\n",
      "|       city_name|airport_code|total_foreigners|\n",
      "+----------------+------------+----------------+\n",
      "|        new york|         NYC|          389149|\n",
      "|           miami|         MIA|          286905|\n",
      "|     los angeles|         LOS|          247509|\n",
      "|   san francisco|         SFR|          137126|\n",
      "|newark/teterboro|         NEW|          131263|\n",
      "+----------------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Cities with most foreigner arrival:\")\n",
    "spark.sql(query_GetTop5CitiesByForeignerCount).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(query_GetCityTemperature).createOrReplaceTempView('City_Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(query_GetCityAverageTemperature).createOrReplaceTempView('City_Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(query_GetCityImmigrationByTemperature).createOrReplaceTempView('Immigration_City_Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foreigner travel to city vs temperature:\n",
      "+----------------+------------------+----------------+\n",
      "|       city_name|       temperature|total_foreigners|\n",
      "+----------------+------------------+----------------+\n",
      "|        new york| 9.409092592592593|          389149|\n",
      "|           miami|22.989722222222216|          286905|\n",
      "|     los angeles|14.007444444444445|          247509|\n",
      "|   san francisco|13.686240740740741|          137126|\n",
      "|newark/teterboro|              null|          131263|\n",
      "+----------------+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Foreigner travel to city vs temperature:\")\n",
    "spark.sql(query_GetTop5CityImmigrationByTemperature).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Data quality checks performed:\n",
    " * Primary key constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " \n",
    "Run the following checks:\n",
    "- Immigration by gender\n",
    "- Immigration by age\n",
    "- Total number of cities where foreigners arrived\n",
    "- Top 5 cities where most foreigners arrived\n",
    "- Foreigner count by visa\n",
    "- Cities with most foreigner arrival\n",
    "- Foreigner travel to city vs temperature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Immigration Table Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality check passed: Immigration Table\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "fact_i94ImmigrationDataPath = parquet_files_directory + 'fact_i94ImmigrationData.parquet'\n",
    "df_Fact_i94ImmigrationData = spark.read.parquet(fact_i94ImmigrationDataPath)\n",
    "originalCountImmi = df_Fact_i94ImmigrationData.count()\n",
    "immi_primary_key = ['i94immi_cicid']\n",
    "afterDropDupliCountImmi = df_Fact_i94ImmigrationData.dropDuplicates(immi_primary_key).count()\n",
    "if(originalCountImmi != afterDropDupliCountImmi):\n",
    "    print(\"Quality check failed: Immigration Table\")\n",
    "else:\n",
    "    print(\"Quality check passed: Immigration Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Port Table Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality check passed: Port Table\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "dim_portDataPath = parquet_files_directory + 'dim_portData.parquet'\n",
    "df_Dim_portData = spark.read.parquet(dim_portDataPath)\n",
    "originalCountPort = df_Dim_portData.count()\n",
    "port_primary_key = ['port_code']\n",
    "afterDropDupliCountPort = df_Dim_portData.dropDuplicates(port_primary_key).count()\n",
    "if(originalCountPort != afterDropDupliCountPort):\n",
    "    print(\"Quality check failed: Port Table\")\n",
    "else:\n",
    "    print(\"Quality check passed: Port Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Visa Table Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality check passed: Visa Table\n"
     ]
    }
   ],
   "source": [
    "dim_visaDataPath = parquet_files_directory + 'dim_visaData.parquet'\n",
    "df_Dim_visaData = spark.read.parquet(dim_visaDataPath)\n",
    "originalCountVisa = df_Dim_visaData.count()\n",
    "visa_primary_key = ['visa_visatype_code']\n",
    "afterDropDupliCountVisa = df_Dim_visaData.dropDuplicates(visa_primary_key).count()\n",
    "if(originalCountVisa != afterDropDupliCountVisa):\n",
    "    print(\"Quality check failed: Visa Table\")\n",
    "else:\n",
    "    print(\"Quality check passed: Visa Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Foreigner Table Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality check passed: Foreigner Table\n"
     ]
    }
   ],
   "source": [
    "dim_foreignerDataPath = parquet_files_directory + 'dim_foreignerData.parquet'\n",
    "df_dim_foreignerData = spark.read.parquet(dim_foreignerDataPath)\n",
    "originalCountForeigner = df_dim_foreignerData.count()\n",
    "temp_primary_key = ['foreigner_cicid']\n",
    "afterDropDupliCountForeigner = df_dim_foreignerData.dropDuplicates(temp_primary_key).count()\n",
    "if(originalCountForeigner != afterDropDupliCountForeigner):\n",
    "    print(\"Quality check failed: Foreigner Table\")\n",
    "else:\n",
    "    print(\"Quality check passed: Foreigner Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Address Table Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality check passed: Address Table\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "dim_addressDataPath = parquet_files_directory + 'dim_addressData.parquet'\n",
    "df_Dim_addressData = spark.read.parquet(dim_addressDataPath)\n",
    "originalCountAddress = df_Dim_addressData.count()\n",
    "address_primary_key = ['address_state_code']\n",
    "afterDropDupliCountAddress = df_Dim_addressData.dropDuplicates(address_primary_key).count()\n",
    "if(originalCountAddress != afterDropDupliCountAddress):\n",
    "    print(\"Quality check failed: Address Table\")\n",
    "else:\n",
    "    print(\"Quality check passed: Address Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "##### Details of the tables\n",
    "\n",
    "See diagram Capstone_DataModel.png\n",
    "See PDF for Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    * Spark Dataframes provide the ability to work with multiple data formats. It is also easy to use and has APIs for python. We can also convert easily from pandas to spark dataframes.\n",
    "    * Spark SQL is easy to use and can be easily integrated\n",
    "    * Pandas is made for python and pandas dataframe can be easily converted to spark dataframes.\n",
    "* Propose how often the data should be updated and why.\n",
    "    * One use case for the data collected here is to track tourism by city. This will help monitor tourism policies,  city development & planning. For this use case, an annual update of this data is sufficient\n",
    "    * For the use case of airlines using this information to plan their flight schedules, an annual update of this data is sufficient.\n",
    "    * For use cases like tracking individuals a daily update of this information may be useful\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "     * We can use cloud based technologies to scale. We can use Redshift storage optimized nodes.  \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     * We can use Airflow to monitor and schedule workflows. To update the dashboard on a daily basis at a specific time, we can create a DAG and schedule it to be run daily at a given start time. We can create an operator in the DAG to perform the task to populate the dashboard\n",
    " * The database needed to be accessed by 100+ people.\n",
    "     * We can use distributed file systems and distributed databases to scale as users increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
